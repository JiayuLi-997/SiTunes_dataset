# SiTunes Dataset: Situational Recommendation Experiment

This repository provides instructions to reproduce the situational recommendation experiment using the SiTunes dataset and RecBole.

## Step 1: Generate Datasets

To create the 30 different datasets, run the `dataloader.py` script located at:
`SiTunes_dataset/Experiments/Situational_recommendation/dataset/dataloader.py`


30  dataset splits will for 10 different random seeds, ranging from 101 to 110 for each of the 3 settings will be generated in the 
`SiTunes_dataset/Experiments/Situational_recommendation/dataset/` folder

## Step 2: Install RecBole

Download RecBole into the `SiTunes_dataset/Experiments/` directory using the following command:

<pre>
git clone https://github.com/RUCAIBox/RecBole.git && cd RecBole
</pre>


Alternatively, you can follow other installation methods available in the [RecBole documentation](https://recbole.io).

## Step 3: Copy Datasets and Configs

Copy the `dataset` and `Configs` folders to the installed RecBole repository for convenience:

- Dataset folder: `SiTunes_dataset/Experiments/Situational_recommendation/dataset/`
- Configs folder: `SiTunes_dataset/Experiments/Situational_recommendation/Configs/`

Configs are created for each experiment setting and the 4 models used in the experiment.

## Step 4: Run the Experiment

To run the experiment, follow these steps:

1. Choose a desired model and setting (also consider with/without situation).
2. Find the correspondig config file.
3. For each run, select the appropriate dataset and modify the seed in the corresponding config file according to the selected model over 10 random seeds [101,110].
4. Take the average result of each evaluation metric produced as output of the 10 runs.

Please make sure to update the dataset path and the config file path in your RecBole experiment commands accordingly.


### Task Settings

We hypothesize that incorporating information about the user's situation could lead to better recommendations. To test this hypothesis, we designed a series of experiments using the three-stage dataset, which is partitioned as follows:

1. **Setting 1:** Stage 1 is used for training, and Stage 2 is used for testing. This serves to predict situational preferences based on inherent preferences to explore the relation and differences between two types of preferences.

2. **Setting 2:** Stage 2 data is used for both training and testing. It is conducted with and without situational information to compare the impact of situational information on predicting preferences.

3. **Setting 3:** Stage 2 data is used for training, and Stage 3 data is used for testing. It is conducted both with and without situational information. This setting helps us examine potential connections and distribution differences of interactions generated by traditional and situational recommenders.

### Data Pre-processing

We take interaction data with user IDs, item IDs, and item metadata. Interactions with rating â‰¥3 are treated as positive ones. We only use automatically generated situations here, and emotion-related features are not considered, as they are subjective information provided by the user. For Stage 2 and Stage 3, we merged interaction data with environmental and physiological situations. The sequential physiological data of 30 minutes was transformed into the mean and standard deviation of each feature, except for activity type, where the major activity type was used. Discrete features are transferred to one-hot embedding, including activity types, weather types, and time periods.

### Baseline Models

As a preliminary experiment on the situational recommendation with *SiTunes*, we select some existing context-aware models for experiments, In our study, we chose to evaluate a set of models, including Factorization Machines (FM), Wide & Deep, AutoInt, and Deep & Cross Network Version 2 (DCN V2). Wide & Deep and FM are selected as they represent classical and popular baseline approaches in the contextual recommendation. And AutoInt and DCN V2 represent recent advances by which we aim to assess the possibility of employing state-of-the-art approaches on *SiTunes*. In experiments, we adopted the popular RecBole framework to implement and evaluate all results.

### Experimental Settings

As mentioned above, we have three settings for the experiments. To make the performance among settings comparable, we apply the following strategy for dataset split: In Setting 1, we randomly split Stage 1 data by users into 80% training and 20% validation, and use 20% of Stage 2 data for testing. For Setting 2, we randomly partition Stage 2 data by users into 70% training, 10% validation, and 20% testing, with test sets identical to those in Setting 1 for a fair comparison. In Setting 3, we used the same training and validation data as in Setting 2, and employed the entire Stage 3 interaction data for testing. We repeatedly generate datasets in each setting with 10 different random seeds and report the average results to reduce data bias.

We evaluate the performance of all models with three point-wise evaluation metrics: AUC, MAE, and RMSE. Point-wise rather than ranking-based evaluation is adopted since RecBole only supports point-wise evaluations for context-aware models with features dynamically changed between interactions. Furthermore, the dynamic situation context is usually used for Click-Through-Rate prediction in practice (and also in the baseline methods we applied), so point-wise metrics are more suitable for comparison.

Results for the music recommendation task. We repeat each experiment 10 times with different random seeds and report the average score. Relative t-test is conducted between results with and without situation data for the same model with */** for p-value<0.05/0.01. Point-wise evaluation metrics have been used due to the restriction of contextual recommendation methods in the RecBole framework

![Experiments results situation](/log/_static/Situational_recommendation_experiments_results.png)

### Experimental Results

The experimental results presented in the referenced table offer compelling evidence for the importance of situational information in recommender systems. Comparing with recommendation results without situation, significant performance improvements are achieved for almost all metrics for all models when situational data is incorporated. Therefore, the situational information provided in *SiTunes* is significantly helpful for music recommendation tasks. The observed improvement in AUC, MAE, and RMSE metrics when situational data is used highlights the significance of this information in enhancing the accuracy of rating predictions.

Comparing results in three settings, the superior performance of models in Setting 2 to Setting 1 illustrates the necessity of real-world data to better understand the impact of situational factors on user preferences, which confirms the need to involve a field study. Performance in Setting 3 is also worse than in Setting 2. The performance decrease may be caused by the distribution discrepancy between Stage 2 and Stage 3 with different backbone recommenders. Nevertheless, it is worth noting that they are not so comparable as the test sets are distinct.

Furthermore, we observe no significant performance difference between the four models. The limited scale of our dataset may cause AutoInt and DCN V2 to not have outstanding performances compared with traditional models. However, these methods are all used for general context-aware recommendation, and we believe models designed explicitly for situational recommendation will lead to better performance in the future, suggesting that our dataset might not be sufficiently large to exploit the capabilities of advanced machine learning methods fully. We believe that a larger dataset with rich context and situational information could better support these methods and yield more insightful results.
